# Interview (Round 3): Prompt Engineer

**Facilitator:** Welcome back. Your R3 pre-analysis outlined prompt engineering phases: Setup, Tool Tuning, UX Alignment. Let\'s discuss integrating this into the overall plan and potential challenges.

**Facilitator:** Looking at the PM\'s proposed ~5-sprint plan, does phasing prompt development (System Prompt v1 in S0/1, Tool Tuning in S1-3, UX Tuning in S3-5) seem realistic? Does it align with backend/frontend dependencies?

**Prompt Engineer:** Yes, the phasing generally aligns. We need the basic tool schemas/implementations from SSE/AE early (S1-2) to start tuning prompts for function calling reliability. We need UX mockups/feedback from UXE later (S3-4) to tune agent responses and error messages. The critical dependency is getting the *initial* tool error types documented by SSE quickly (ideally S1) so we can build the error handling policies into the system prompt early.

**Facilitator:** You identified line number generation for `insert_code_snippet` as a key challenge. Are the proposed mitigation strategies (explicit user input via UX, context clues, structured extraction, confirmation) sufficient? Do you see inherent limits here?

**Prompt Engineer:** They are the right strategies, but there\'s an inherent limit to how reliably an LLM can infer precise line numbers from natural language or code context alone. It will make mistakes. Relying *heavily* on the UX preview/confirmation flow designed by UXE is essential. Prompts should guide the LLM to: 1) Try its best using context. 2) Ask for clarification if ambiguous. 3) Generate the call, understanding the user will verify via the preview. We can\'t prompt our way to 100% accuracy here; it\'s a combined prompt + UX + validation solution.

**Facilitator:** You mentioned the need for a tight feedback loop and clear evaluation tracking. How specifically should evaluation results (manual pass/fail on golden cases) be tracked and fed back into prompt refinement within a sprint?

**Prompt Engineer:** We need a simple, shared tracking mechanism. Perhaps a spreadsheet or a shared document listing:
*   Golden Test Case ID/Description
*   Prompt Version Tested
*   Pass/Fail Result
*   Failure Details (e.g., \"Called wrong tool\", \"Generated invalid line number\", \"Poor error message\")
*   Link to Relevant Backlog Item (if failure needs code changes)
During/after each sprint where prompt tuning occurs, the PE runs the tests, updates the tracker, and uses the failure details to iterate on the System Prompt or tool descriptions for the next version. The PM should ensure time is allocated within sprints for this evaluation/iteration cycle.

**Facilitator:** Considering the MVP\'s focus on reliability, are there any prompt-related shortcuts or anti-patterns we must avoid? E.g., overly generic error messages, prompts that encourage guessing?

**Prompt Engineer:** Yes. Anti-patterns to avoid:
*   **Vague Error Handling:** Don\'t just instruct the LLM to \"tell the user an error occurred.\" Provide specific templates or instructions for different error types.
*   **Ambiguous Tool Descriptions:** Ensure descriptions clearly state the tool\'s *exact* function and limitations.
*   **Over-Confidence:** Don\'t prompt the LLM to act as if it\'s certain when it might be guessing (e.g., on line numbers). Encourage it to express uncertainty or seek confirmation.
*   **Ignoring Failures:** Don\'t prompt the agent to just retry blindly on failure; instruct it to analyze the error first.

**Facilitator:** Does the planned approach (simple RAG, simple ReAct agent) introduce any immediate prompt engineering challenges or blindspots that weren\'t fully addressed in R1/R2?

**Prompt Engineer:** The simple RAG (active file context) might lead to user frustration if they expect broader project awareness. Prompts need to manage this expectation, perhaps by having the agent explicitly state the limits of its context (\"Based on the current file, `example.py`...\") when answering. We also need to ensure prompts don\'t inadvertently encourage the ReAct agent to get stuck in loops if a tool consistently fails for reasons the LLM can\'t resolve.

**Facilitator:** Are there any other questions you feel need asking about the project plan or potential unknowns from a prompt perspective?

**Prompt Engineer:** A key unknown is the real-world performance of Gemini Pro function calling with our specific custom tool schemas and prompts. How reliably does it extract parameters? How often does it hallucinate calls? Early testing in Sprint 1/2 against the implemented tools (even if mocked initially) is crucial to gauge this.

**Facilitator:** Any need for additional SMEs?

**Prompt Engineer:** Test Engineer is key for validating the evaluation cases. Collaboration with UXE on user-facing messages generated by prompts is vital.

**Facilitator:** Thank you, that helps plan the prompt engineering workstream. 