---
persona: Prompt Engineer
date: 2025-04-26
interview_focus: Exploring AAI vs CAB for minimizing copy/paste via prompt design.
---

## Prompt Engineer - Simulated Interview

**Facilitator:** Thanks for joining, Prompt Engineer. We're digging into the feedback about minimizing copy/paste in the MotM process. Your pre-analysis focused on leveraging the Assistant (AAI) or the clipboard (CAB). Let's explore the AAI approach first – instructing the Assistant via generated prompt files. What inherent challenges do you see there?

**Prompt Engineer:** The biggest challenge is the Assistant's reliability and interpretation fidelity. We're asking it to perform a sequence: read file A (instruction prompt), potentially read file B (context mentioned in A), formulate a new prompt for its internal LLM, get the response, and then write that *entire* response to file C. That's a complex chain. If any step fails or gets misinterpreted – maybe it only saves a summary, or fails to read the context file – the whole process breaks silently. We need prompts that are incredibly precise instructions *for the Assistant*, not just for the underlying LLM.

**Facilitator:** So, the friction point is ensuring the Assistant follows the file I/O and chaining instructions perfectly? Are there hard limits, like the Assistant refusing certain file operations or misunderstanding complex instruction sequences?

**Prompt Engineer:** Exactly. Hard limits could be the Assistant's tool limitations (can it *actually* write its last response reliably?), context window limits for the *instruction prompt* itself (if it gets too complex), or simply inconsistent behavior in following chained instructions. We'd need empirical testing to know for sure. The prompt needs to be unambiguous: "Read file X. Read file Y. Combine their content with the following text Z. Submit the combination to your LLM. Save the complete, verbatim response to file P."

**Facilitator:** If AAI proved reliable, how would your solution look? What would the prompts generated by the orchestrator script look like?

**Prompt Engineer:** The script would generate a Markdown file, say `assistant_instruction_step_N.md`. Its content wouldn't be the *final* LLM prompt, but the instructions *for the Assistant*. Like:
```markdown
**ASSISTANT TASK**

1.  **Read Context:** Load the content from `../state/previous_round_output.md`.
2.  **Read Task:** Use the task description provided below.
3.  **Combine & Prompt LLM:** Create a single prompt for your internal LLM by combining the context from step 1 and the task from step 2, using the template structure defined in `../../prompts/internal_llm_template.md`.
4.  **Execute Prompt:** Submit the combined prompt to your LLM.
5.  **Save Output:** Save the *entire, unmodified* response you receive from the LLM to the file `../state/current_round_output.md`.
6.  **Confirm:** Report back "Task complete: Output saved to current_round_output.md" or report any errors encountered.

**TASK DESCRIPTION:**
[... Specific task for this MotM step ...]
```
The user would then just tell the Assistant: "Process the instructions in `brain/knowledge/chronological/2025/04/26/MotM/round-1/feedback/pre-analysis/assistant_instruction_step_N.md`".

**Facilitator:** That's clear. Are there unknown unknowns here? Questions we need to answer before committing to AAI?

**Prompt Engineer:** Definitely. We *need* to know:
1.  Does the Assistant *currently* have robust tools for reading arbitrary files specified in a prompt?
2.  Does it have a tool to save its *own last response* to a specified file, not just execute code that writes a file?
3.  How consistently does it follow multi-step instructions involving file I/O?
4.  What are its error reporting capabilities if a step fails? Can the script detect failure?

**Facilitator:** Good points. Does the feedback (hating copy/paste) have any blind spots from a prompt engineering perspective?

**Prompt Engineer:** The main blind spot might be underestimating the complexity of *instructing the Assistant* reliably. It shifts the burden from user C/P to precise meta-prompting for the Assistant. If that meta-prompting becomes too complex or brittle, the user might trade one frustration for another (debugging Assistant instructions). Also, the CAB approach, while involving C/P, might be more robust if the LLM prompts themselves are designed to request easily copyable, single-block outputs.

**Facilitator:** Finally, based on this focus on Assistant interaction and meta-prompting, are there any other SMEs who should be involved?

**Prompt Engineer:** An **AI Agent Engineer** is crucial, as they focus on orchestrating steps and tool usage. And perhaps someone deeply familiar with the *specific Assistant's capabilities and limitations* within Cursor (if that's a distinct role, maybe a Cursor Platform Expert?). The AI UX Engineer is also vital to ensure the *instructions given to the user* for directing the Assistant are clear and minimize friction.

**Facilitator:** Excellent. Thanks for your insights. That gives us a lot to work with. 