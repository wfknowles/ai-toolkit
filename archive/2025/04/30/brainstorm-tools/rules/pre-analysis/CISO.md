# CISO - Pre-Analysis & Concepts (Cursor Rules)

**Based on:** Ensuring security policy adherence, managing sensitive data exposure risk within the IDE, promoting secure coding practices, and maintaining auditability of AI-assisted development.

**Goal:** Propose Cursor rules that enforce security policies, prevent accidental leakage of sensitive information, and align AI assistant behavior with organizational security standards.

**Initial Concepts (7 Rules):**

1.  **Rule: Prohibit Sensitive File Inclusion in Prompts:**
    *   **Level:** Project/User
    *   **Description:** Prevent the AI assistant from automatically including files matching certain patterns (e.g., `*.key`, `credentials.yml`, `secrets.json`, files in `.git/`) or files explicitly marked with a `// SENSITIVE` comment in its context/prompts unless explicitly overridden by the user for a single query.
    *   **Rationale:** Reduces risk of accidentally sending secrets or sensitive configuration to the LLM.
2.  **Rule: Mandate Security Linter Integration:**
    *   **Level:** Project
    *   **Description:** Ensure that a security linter/SAST tool (e.g., Semgrep, Bandit) is configured for the project. Potentially instruct the AI assistant to prioritize fixing high-severity findings from this tool when asked to refactor or fix code.
    *   **Rationale:** Enforces baseline static analysis security checks.
3.  **Rule: Restrict AI Access to Specific Network Endpoints:**
    *   **Level:** User/Project
    *   **Description:** If the AI assistant has capabilities involving network access (e.g., via tools), define an allowlist or denylist of network endpoints it can interact with. Prevent interaction with internal production systems or known malicious domains.
    *   **Rationale:** Limits the potential impact of a compromised or misused AI tool.
4.  **Rule: Enforce Audit Logging for AI Actions:**
    *   **Level:** User/Project
    *   **Description:** Ensure detailed logs are kept (locally or sent to a central store if possible) for significant AI actions, such as file modifications, tool usage, or potentially sensitive data access attempts. Include user, timestamp, files involved, and summary of action.
    *   **Rationale:** Provides audit trail for security reviews and incident response.
5.  **Rule: Discourage Generation of Hardcoded Secrets:**
    *   **Level:** Project/User
    *   **Description:** Instruct the AI assistant to avoid generating code that includes hardcoded secrets (API keys, passwords). When generating code that requires secrets, it should use placeholders and remind the user to use a secure secret management solution.
    *   **Rationale:** Promotes secure coding practices regarding secrets.
6.  **Rule: Contextual Security Reminders:**
    *   **Level:** User
    *   **Description:** When the user includes potentially sensitive files (even if allowed by Rule 1 override) or asks the AI to perform actions on security-sensitive code (e.g., authentication logic), inject a brief, non-blocking reminder about security best practices or organizational policy links into the chat.
    *   **Rationale:** Raises security awareness contextually during development.
7.  **Rule: Validate AI-Generated Infrastructure Code:**
    *   **Level:** Project
    *   **Description:** If using the AI to generate Infrastructure as Code (IaC, e.g., Terraform, CloudFormation), instruct it to adhere to predefined security templates or run generated code through an IaC security scanning tool (e.g., Checkov, tfsec) before finalizing.
    *   **Rationale:** Prevents deployment of insecure infrastructure configurations generated by AI. 