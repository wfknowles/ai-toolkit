# Interview Transcript: Senior Software Engineer

**Date:** 2025-05-02
**Persona:** Senior Software Engineer (SSE)
**Interviewer:** Facilitator

**Facilitator:** Thanks, SSE. Your focus on practical SE applications is vital. What challenges do you anticipate in getting ~200 engineers with varying backgrounds to effectively apply these prompt techniques in their day-to-day work using Cursor?

**SSE:** The main challenge is relevance and habit formation. The techniques need to demonstrably solve *their* problems – debugging faster, writing boilerplate code, generating unit tests, explaining legacy code, etc. If the examples feel too academic or disconnected from their actual codebase types or tasks, adoption will lag. We also need to overcome inertia – it takes effort to switch from googling or manual coding to structured prompting.

**Facilitator:** Which parts of the curriculum seem most cognitively demanding for a typical software engineer?

**SSE:** Anything abstract. The core techniques in Unit 2 are fine if taught with good examples. Unit 3, especially designing robust chains or understanding RAG implementation details (even conceptually), requires more abstract thinking. Unit 4's agent loops and meta-prompting might feel quite alien initially. Grounding these in concrete SE tasks (e.g., "Use an agent loop to refactor this code based on guidelines") is essential.

**Facilitator:** What were your "Aha!" moments using LLMs/prompting in your SE work?

**SSE:** Seeing the LLM generate surprisingly accurate boilerplate code (e.g., API client setup) from a simple natural language request. Using it to explain a complex piece of unfamiliar code was a game-changer. Also, getting it to generate decent unit tests for edge cases I hadn't considered.

**Facilitator:** Any blindspots engineers might bring?

**SSE:** Treating the AI like a magic black box – pasting code and expecting perfect results without understanding *how* to guide it. Not verifying the AI's output thoroughly – it hallucinates code too! Also, perhaps not thinking about how to integrate prompting into existing development workflows (version control, code reviews for prompts?).

**Facilitator:** How do your lesson ideas contribute?

**SSE:** Strength: Directly linking prompting techniques to common SE tasks (debugging, testing, documentation, refactoring - Lessons 2.4.1, 3.5.1). Focusing on practical Cursor usage. Weakness: Might need PE/AOA input to ensure the *techniques* are taught rigorously alongside the *application*.

**Facilitator:** Where can you best contribute during research?

**SSE:** Identifying high-value SE use cases for each technique, providing realistic code examples and scenarios for labs/demos, defining requirements for Cursor integration examples (2.4.1), contributing to the capstone project definition (Unit 5) to ensure it's a relevant SE task.

**Facilitator:** And during development?

**SSE:** Reviewing examples and exercises for relevance and realism, helping translate theoretical concepts into practical SE actions, ensuring the language and framing resonates with engineers, perhaps helping develop sample codebases for exercises.

**Facilitator:** Other SMEs needed for roadmapping?

**SSE:** PE for the core techniques, AI UX for smooth Cursor integration, PO to ensure alignment with overall developer productivity goals.

**Facilitator:** Anything else crucial?

**SSE:** We need to address the "when *not* to use AI" aspect. Sometimes traditional methods are faster or more reliable. Also, managing expectations – it's a powerful tool, but not a replacement for critical thinking or fundamental SE skills. The course should position it as an augmenter, not a replacement.

 