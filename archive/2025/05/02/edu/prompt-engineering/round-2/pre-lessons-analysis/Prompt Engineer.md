# Pre-Lesson Analysis: Prompt Engineer

**Focus Areas (Curriculum v2):**
*   Unit 2: Core Prompt Craft (Modules 2.1, 2.3, 2.4)
*   Unit 3: Building Complexity (Modules 3.1, 3.3, 3.4 - evaluation aspect)
*   Unit 4: Advanced Techniques (Modules 4.1, 4.4 - evaluation/security)
*   Cross-Cutting: Iterative Refinement & Prompt Debugging, Cursor Integration

**Research Notes for RAs:**
*   **Module 2.1 (Few-Shot):** Find recent examples/case studies of few-shot prompting specifically for *code refactoring* and *test generation* within IDE environments (like Cursor/Copilot). Look for patterns that work well for Python/TypeScript.
*   **Module 2.4 (Debugging):** Compile a list of the top 5-10 *most common prompt failures* engineers encounter when using AI for coding tasks (e.g., vague instructions, context gaps, format issues). Find examples of systematic debugging workflows specifically for prompts (beyond generic advice). Any tools/techniques for visualizing prompt execution flow?
*   **Module 3.1 (CoT):** Research best practices for prompting CoT specifically for *code explanation* and *complex bug diagnosis*. Are there optimal phrasing patterns ("think step-by-step" vs. others)?
*   **Module 4.1 (Advanced):** Find practical examples/implementations of *Self-Consistency* applied to code generation or analysis tasks solvable within a Cursor extension context. How can Meta-Prompting be realistically demonstrated/used by engineers in Cursor?
*   **Module 4.4 (Security):** Gather latest research/examples on prompt injection attacks specifically targeting *code generation workflows* or IDE plugins. Best mitigation techniques applicable *at the prompt level*? 