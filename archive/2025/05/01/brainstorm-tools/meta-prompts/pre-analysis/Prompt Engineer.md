# Prompt Engineer - Brainstorming Pre-Analysis: Meta-Prompts & Modularity

**Focus:** Advanced Prompting Techniques for Software Maintenance Automation

**Concepts Brainstormed (9):**

1.  **Layered Prompt Architecture:** Define prompts in layers: a base layer for core instructions (e.g., "Analyze this code"), a context layer dynamically injected (code snippets, docs, error logs), a task-specific layer (e.g., "Identify security vulnerabilities," "Suggest refactoring"), and a formatting layer (e.g., "Output as JSON," "Provide a concise summary"). Orchestrator combines layers based on the specific task.
2.  **Config-Driven Prompt Generation:** Store prompt fragments, templates, and parameters in configuration files (YAML/JSON). A "Prompt Assembler" reads the config for a given task (e.g., `code_review_security`) and dynamically constructs the final prompt by selecting and combining relevant fragments, injecting variables, and applying formatting rules.
3.  **Chain-of-Thought with Tool Schema Injection:** For multi-step tasks (e.g., `update dependency -> run tests -> analyze failure`), use chain-of-thought prompting. Critically, at each step where the AI might need external info, dynamically inject the *schema* or description of available tools/adapters (e.g., "You can use the `TestRunnerAdapter` with schema `{'run_tests': ...}`"). This helps the AI formulate requests for the *right* tool with the *right* parameters.
4.  **Persona-Driven Meta-Prompt:** A meta-prompt that takes a target persona ("Junior Dev," "Senior Architect," "Security Auditor") and a core task instruction as input. The meta-prompt instructs the LLM to *first* adopt the specified persona and *then* execute the task, tailoring the explanation level, focus (e.g., simplicity vs. performance trade-offs), and output format accordingly.
5.  **Self-Correction/Refinement Loop Prompt:** For tasks like generating documentation or code comments, use a multi-turn prompt loop. Turn 1: Generate initial draft. Turn 2: Present the draft back to the LLM with instructions like "Critique the previous output for clarity, accuracy, and completeness based on [Style Guide/Code Context]. Suggest specific improvements." Turn 3: Instruct the LLM to generate a revised version incorporating the critique.
6.  **Context Prioritization Prompting:** When context is large (e.g., multiple files for refactoring), use prompt instructions that guide the LLM on how to prioritize context: "Focus primarily on functions X and Y in `fileA.py`. Use `fileB.py` only for understanding class Z definitions. Ignore `tests/` directory context unless explicitly needed for test analysis." Requires careful context chunking/labeling by the orchestrator.
7.  **"Explain the Trade-offs" Meta-Instruction:** Add a standard meta-instruction to prompts generating suggestions (refactoring, updates): "For each suggestion, briefly explain the primary trade-offs involved (e.g., performance vs. readability, potential risks vs. benefits)." Forces more nuanced output.
8.  **Example-Driven Prompt Generation (Few-Shot Dynamic Selection):** Maintain a library of high-quality input/output examples for various tasks. For a new task, the orchestrator retrieves the N *most semantically similar* examples from the library and injects them into the prompt as few-shot demonstrations before the actual request. Makes prompts more adaptive without manual template changes.
9.  **Prompt Template Inheritance/Composition:** Define base prompt templates (e.g., `base_code_analysis.prompt`) and allow task-specific templates to "inherit" or "include" fragments from base templates. E.g., `security_scan.prompt` could include `base_code_analysis.prompt` and add security-specific instructions and output requirements. Reduces redundancy. 