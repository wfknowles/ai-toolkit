# Our Project: Secure Coding Standards & AI Code Review Checklist - v1.0

## 1. Purpose

This document outlines the essential secure coding standards expected for all code contributed to Our Project, whether written by human developers or generated with the assistance of AI Large Language Models (LLMs). It also provides a specific checklist for reviewing code generated by AI to ensure it meets our security and quality bar.

Adherence to these standards aims to minimize security vulnerabilities, improve code robustness, and ensure compliance with best practices, including principles aligned with the OWASP Top 10. This is a living document and will be updated regularly. All developers are expected to be familiar with and apply these standards.

**Related Documents:**
*   [Our Project: Prompt Engineering Standards](link_to_component_1.md)
*   [Developer Onboarding Guide](link_to_component_12.md)

## 2. General Secure Coding Principles (Apply to All Code)

These principles should guide all development work:

*   **Input Validation:** Never trust external input (from users, APIs, files, environment variables, databases, or even AI-generated content). Validate all input for type, length, format, and range. Use allow-lists (whitelisting) over block-lists (blacklisting) where possible.
*   **Output Encoding:** Encode all output appropriately for the context in which it will be rendered or interpreted (e.g., HTML encoding for web display, proper escaping for OS commands, escaping for log messages) to prevent injection attacks (XSS, Log Injection, etc.).
*   **Principle of Least Privilege:** Code should run with the minimum permissions necessary to perform its function. Service accounts, API keys, and database connections should have narrowly scoped permissions.
*   **Defense in Depth:** Implement multiple layers of security controls. Don't rely on a single check.
*   **Secure Error Handling:** Avoid revealing sensitive information (e.g., stack traces, system details, internal paths) in error messages shown to users. Log detailed error information securely on the backend. Implement robust try-catch blocks or equivalent error handling.
*   **Secure Dependency Management:** Keep all third-party libraries and frameworks up-to-date. Regularly scan for vulnerabilities using SCA tools (integrated via Component #7). Only use dependencies from trusted sources.
*   **Authentication and Authorization:** Implement strong authentication mechanisms. Ensure authorization checks are performed on every sensitive operation, verifying the user has the right permissions *after* they have authenticated.
*   **Secrets Management:** Never hardcode secrets (API keys, passwords, tokens, certificates) in source code, configuration files, or logs. Use approved secrets management solutions (e.g., HashiCorp Vault, AWS Secrets Manager, Azure Key Vault) accessed via secure configurations or IAM roles.
*   **Secure Logging:** Log security-relevant events (e.g., logins, failed logins, access control failures, significant transactions), but avoid logging sensitive data (passwords, PII, full credit card numbers) unless absolutely necessary and properly secured/masked. Protect log files from unauthorized access or modification.

## 3. Language-Specific Secure Coding Guidance (Examples)

*(Note: This section would be expanded with detailed, specific guidance for each primary language used in the project. Below are illustrative examples.)*

### 3.1 Python Specific Guidance
*   **SQL Injection:** Always use parameterized queries or well-vetted Object-Relational Mappers (ORMs) like SQLAlchemy or Django ORM. Never use string formatting (f-strings, `.format()`, `%`) to construct SQL queries with user input.
*   **Command Injection:** Avoid using `os.system()`, `subprocess.call(shell=True)`, or similar functions with unvalidated input. Use `subprocess` functions with `shell=False` and pass arguments as a list. Sanitize inputs rigorously if dynamic command generation is unavoidable.
*   **Deserialization:** Avoid using `pickle` with untrusted data. Use safer serialization formats like JSON or YAML with appropriate libraries (e.g., `PyYAML` with `safe_load`).
*   **Cross-Site Scripting (XSS):** When using web frameworks (Django, Flask, FastAPI), leverage their built-in templating engines (e.g., Jinja2) which perform auto-escaping by default. Be cautious when disabling auto-escaping or manually constructing HTML. Sanitize user input displayed in web pages.
*   **Directory Traversal:** Sanitize user-supplied filenames and paths. Use `os.path.abspath()` and check if the resulting path is within an allowed base directory.

### 3.2 JavaScript (Node.js / Frontend) Specific Guidance
*   **XSS:** Use frameworks (React, Vue, Angular) that provide context-aware auto-escaping. When directly manipulating the DOM, use `.textContent` instead of `.innerHTML` where possible. Use trusted types if available. Sanitize HTML input using robust libraries (e.g., DOMPurify).
*   **Server-Side Request Forgery (SSRF):** Validate and sanitize URLs provided by users before making server-side requests (e.g., using `fetch` in Node.js). Use allow-lists for target domains/IPs if possible.
*   **Prototype Pollution:** Be cautious when merging or cloning objects recursively, especially with user-provided input. Use libraries designed to prevent prototype pollution or carefully validate keys.
*   **Regular Expression Denial of Service (ReDoS):** Write efficient regular expressions and avoid nested quantifiers with overlapping patterns that can lead to catastrophic backtracking, especially when processing user input. Use linting rules or specialized tools to detect potential ReDoS patterns.
*   **Insecure Dependencies:** Use `npm audit` or integrated SCA tools regularly to check for vulnerabilities in Node.js dependencies. Keep `package-lock.json` committed and use `npm ci` in builds.

*(Add sections for other relevant languages like Java, Go, C#, etc.)*

## 4. Guidance for Prompting AI for Secure Code Generation

Referencing the [Prompt Engineering Standards](link_to_component_1.md), when prompting LLMs to generate code, pay specific attention to:

*   **Explicit Security Requirements:** Include security requirements directly in the prompt.
    *   *Example:* "Generate a Python Flask route that accepts user input 'name'. Ensure the input is validated to prevent XSS and that the response is properly HTML encoded."
*   **Request Specific Secure Libraries/Functions:** Instruct the LLM to use known secure libraries or functions.
    *   *Example:* "Write a Java function to execute this SQL query using JDBC PreparedStatement to prevent SQL injection."
*   **Specify Error Handling Needs:** Ask for robust error handling to be included.
    *   *Example:* "Include appropriate try-catch blocks and log errors securely without exposing sensitive details."
*   **Provide Secure Context/Examples:** If using few-shot prompting, ensure the examples provided demonstrate secure coding practices.

**Crucially:** Never assume AI-generated code is secure by default, regardless of how well-prompted.

## 5. Checklist for Reviewing AI-Generated Code

This checklist should be used by human reviewers in addition to standard code review practices and automated scanning results when evaluating AI-generated code contributions:

*   **[ ] Functionality:** Does the code correctly implement the requested functionality as specified in the prompt/task?
*   **[ ] Input Validation:** Are all external inputs (parameters, data from files/APIs) properly validated (type, length, format, range)? Is allow-listing used where appropriate?
*   **[ ] Output Encoding:** Is all output being sent to other systems (web browsers, databases, command shells, logs) properly encoded/escaped for that context?
*   **[ ] Error Handling:** Is error handling robust? Are errors caught appropriately? Are sensitive details avoided in user-facing errors? Are details logged securely?
*   **[ ] Security Best Practices (Language Specific):** Does the code avoid common pitfalls for the specific language (e.g., SQLi, XSS, Command Injection, Insecure Deserialization)? Refer to Section 3.
*   **[ ] Dependency Usage:** Does the code introduce new third-party dependencies? If so, have they been vetted by SCA tools and are their licenses compliant? Does the AI rely on outdated or insecure libraries?
*   **[ ] Secrets Management:** Does the code handle secrets correctly (i.e., does NOT contain hardcoded secrets, reads them from a secure source)?
*   **[ ] Authentication/Authorization Logic:** If the code involves auth checks, are they implemented correctly and consistently checked before sensitive operations?
*   **[ ] Logic and Assumptions:** Does the code make logical assumptions that might be incorrect in certain edge cases? Does the overall logic seem sound? (AI can sometimes produce subtly flawed logic).
*   **[ ] Complexity and Maintainability:** Is the code overly complex or difficult to understand? (AI can sometimes generate convoluted solutions). Does it meet project coding standards and include adequate comments/documentation?
*   **[ ] Testability:** Is the code structured in a way that is reasonably testable? Were appropriate unit tests generated alongside the code (if requested)?
*   **[ ] Alignment with Prompt Intent:** Does the code directly address the core task requested in the prompt, or does it include unnecessary or potentially harmful "extra" functionality?

**Note:** If any checks fail, the code requires revision by the developer (potentially by refining the prompt and regenerating, or by manual correction) before it can be approved.

## 6. Updates and Maintenance

This document will be reviewed and updated quarterly, or more frequently as needed, by the Application Security team (`ai_app_cloud_security_engineer_v1`) in consultation with development leads. Suggestions for improvement are welcome via pull requests to this document.